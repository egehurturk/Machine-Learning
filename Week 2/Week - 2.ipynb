{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------- PART 1: THEORY ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we had the X feature (only 1 feature) whereas in this topic we will have multiple features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Size | # of bedrooms | # of floors | Median Income | Age of home |\n",
    "|------|---------------|-------------|---------------|-------------|\n",
    "| 2104 | 5             | 1           | 5000          | 5           |\n",
    "| 1416 | 3             | 2           | 6000          | 10          |\n",
    "| 1534 | 4             | 1           | 4000          | 4           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x^{(i)} = The\\ input\\ features\\ of\\ the\\ ith\\ training\\ example\\ $(this is an $\\mathbb{R}^{n}$ dimensional vector containing all the features of a row)\\\\\n",
    "$x^{i}_{j} = Value\\ of\\ a\\ feature\\ j\\ in\\ the\\ training\\ example\\ i$ (this is a scalar value)\n",
    "\\\n",
    "$n = Number\\ of\\ features$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} ... + \\theta_{n}x_{n}\n",
    "\\\\\n",
    "=\n",
    "\\\\\n",
    "h_{\\theta}(x) = \\theta^{T}x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a new bias term $x_{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x= \\begin{bmatrix}\n",
    "x_{0}\\\\\n",
    "x_{1}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "x_{n}\\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n+1} \\ \\ \\ \\ \\theta =  \\begin{bmatrix}\n",
    "\\theta_{0}\\\\\n",
    "\\theta_{1}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "\\theta_{n}\\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n+1}\\\\x_{0}=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea is to tweak the parameters iteratively in order to minimize cost function. \n",
    "\n",
    "Gradient Descent measures the local gradient (slope) of the error function with respect to the parameter vector $\\theta$, and it goes in the direction of **descending** gradient. Once the gradient is zero, you've reached a minimum.\n",
    "\n",
    "You first randomly initialize $\\theta$ with random values, then improve it gradually by taking some steps (**learning rate**) to decrease cost function, until the algorithm converges to a minimum. The size of the steps determine the descent. \n",
    "\n",
    "- If $\\alpha$ is too small, it will take a lot of times and iterations to converge\n",
    "- If $\\alpha$ is too large, it will shoot the minimum\n",
    "- The cost function is a **convex function** so that the GD is guaranteed to approach close to global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ repeat\\\\\n",
    "\\theta_{j} = \\theta{j} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)}).x^{(i)}_{j} \\ \\ (for\\ j=1,2,...n)\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key intuition is that we are modifying the parameter based on how far off it is. If our error function gets better when the parameter changes, we change it in the appropriate direction.\n",
    "* Should have a thought experiment. If you overshoot, then reverse it and change the angle. Imagine aiming at a target. If you are off (above) then you have to LOWER (negative above) your angle\n",
    "* You lower proportional to the amount. If you’re off by a lot, you lower by a lot. If you’re off by a little, you change by a little.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Vectorized\\ Implementation\\\\ \\frac{\\partial}{\\partial\\theta_{j}}MSE(\\theta) = \\frac{2}{m}\\sum_{i=1}^{m}(\\theta^{T}x^{(i)}) = y^{(i)}.x^{(i)}_{j}\\\\ \\nabla_{\\theta}MSE(\\theta) = \\begin{pmatrix}\n",
    "\\frac{\\partial}{\\partial \\theta_{0}}MSE(\\theta)\\\\ \n",
    "\\frac{\\partial}{\\partial \\theta_{1}}MSE(\\theta)\\\\ \n",
    ".\\\\ \n",
    ".\\\\ \n",
    ".\\\\ \n",
    "\\frac{\\partial}{\\partial \\theta_{n}}MSE(\\theta)\\\\\n",
    "\\end{pmatrix} = \\frac{2}{m}\\textbf{X}^{T}(\\textbf{X}\\theta - \\textbf{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the gradient vector $\\nabla_{\\theta}$, which points uphill, just go in the opposite direction to go downhill. This means subtracting $\\nabla_{\\theta}$ from $\\theta$. This is where the learning rate $\\alpha$ comes in:\n",
    "$$\\theta^{(next\\ step)} = \\theta - \\alpha \\nabla_{\\theta}MSE(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can speed up our Gradient Descent algorithm by having each of our input values to be on the same range. This is because the GD will descent quickly on small ranges and slowly on large ranges. \n",
    "* Feature scaling speeds up gradient descent by avoiding many extra iterations that are required when one or more features take on much larger values than the rest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feauture Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E.g:\\ x_{1} = size(0-2000\\ feet^{2})\\ \\  Do:\\\\ x_{1} = \\frac{size}{2000} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace $x_{i}$ with $x_{i} - \\mu_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{1} = \\frac{x_{1} - \\mu_{1}}{\\sigma}$$ Where $\\sigma$ is the Standard Deviation ($min - max$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{1}^{2} + \\theta_{4}x_{1}x_{2}...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is very important in Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a closed and mathematical way to perform minimization: $$\\theta = (\\textbf{X}^{T}\\textbf{X})^{-1}\\textbf{X}^{T}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Gradient Descent        | Normal Equation           |\n",
    "|-------------------------|---------------------------|\n",
    "| Need to choose alpha    | No need to choose alpha   |\n",
    "| Needs many iterations   | No iterations             |\n",
    "| Computationally cheaper | Computationally expensive |\n",
    "| Works well even n is large|         Slow if n is very large                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------- PART 2: CODE --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Datasets/50_Startups.csv') # name the dataset\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
